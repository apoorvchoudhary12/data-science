//  after installation of  libraries using "pip install feedparser sqlalchemy celery nltk."
# Import necessary libraries
import feedparser
from sqlalchemy import create_engine, Column, String, Integer, DateTime, create_engine
from sqlalchemy.orm import declarative_base, sessionmaker
from celery import Celery
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk import pos_tag
from nltk import classify
from nltk import NaiveBayesClassifier
import logging
from datetime import datetime

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Set up Celery
app = Celery('news_processor', broker='pyamqp://guest:guest@localhost//')

# Database setup
Base = declarative_base()
engine = create_engine('postgresql://username:password@localhost/news_database')
Base.metadata.create_all(engine)
Session = sessionmaker(bind=engine)

# Model for the News Article
class NewsArticle(Base):
    __tablename__ = 'news_articles'
    id = Column(Integer, primary_key=True)
    title = Column(String)
    content = Column(String)
    pub_date = Column(DateTime)
    source_url = Column(String)
    category = Column(String)

# Function to tokenize and clean text
def clean_text(text):
    stop_words = set(stopwords.words("english"))
    words = word_tokenize(text)
    words = [word.lower() for word in words if word.isalpha()]
    words = [word for word in words if word not in stop_words]
    return words

# Celery task for processing news articles
@app.task
def process_news_article(article_data):
    try:
        # Extract relevant information
        title = article_data['title']
        content = article_data['content']
        pub_date = datetime.strptime(article_data['pub_date'], "%a, %d %b %Y %H:%M:%S %z")
        source_url = article_data['source_url']

        # Tokenize and clean text for classification
        words = clean_text(title + " " + content)

        # Dummy text classification (replace with your own logic)
        category = classify_text(words)

        # Store in the database
        with Session() as session:
            article = NewsArticle(title=title, content=content, pub_date=pub_date, source_url=source_url, category=category)
            session.add(article)
            session.commit()
            logger.info(f"Article stored in the database: {title}")
    except Exception as e:
        logger.error(f"Error processing article: {str(e)}")

# Dummy text classification function (replace with your own logic)
def classify_text(words):
    # Your classification logic here
    return "Others"

# Main script to fetch and process RSS feeds
def fetch_and_process_feeds():
    rss_feeds = [
        "http://rss.cnn.com/rss/cnn_topstories.rss",
        "http://qz.com/feed",
        "http://feeds.foxnews.com/foxnews/politics",
        "http://feeds.reuters.com/reuters/businessNews",
        "http://feeds.feedburner.com/NewshourWorld",
        "https://feeds.bbci.co.uk/news/world/asia/india/rss.xml"
    ]

    for feed_url in rss_feeds:
        feed = feedparser.parse(feed_url)
        for entry in feed.entries:
            article_data = {
                'title': entry.title,
                'content': entry.summary,
                'pub_date': entry.published,
                'source_url': entry.link
            }
            # Send article for processing to Celery
            process_news_article.delay(article_data)

if __name__ == "__main__":
    fetch_and_process_feeds()
